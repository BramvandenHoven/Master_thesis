# -*- coding: utf-8 -*-
"""Master Thesis Bram van den Hoven

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-T6hfA-w_Z27s8_tjXtgJr-uYmBJezXU

# Pre-Processing: .mat to .csv
"""

# Import Required Packages
import numpy as np
from scipy.io import loadmat
import pandas as pd
import datetime as date
from dateutil.relativedelta import relativedelta

# Setting directionary of the data
imdb_mat = '/content/drive/My Drive/Master Data Science & Society/Thesis Data Science & Society/Data/imdb.mat'
wiki_mat = '/content/drive/My Drive/Master Data Science & Society/Thesis Data Science & Society/Data/wiki.mat'

imdb_data = loadmat(imdb_mat)
wiki_data = loadmat(wiki_mat)

# Split data on features for both IMDB and Wiki
imdb = imdb_data['imdb']
wiki = wiki_data['wiki']

imdb_photo_taken = imdb[0][0][1][0]
imdb_full_path = imdb[0][0][2][0]
imdb_gender = imdb[0][0][3][0]
imdb_face_score1 = imdb[0][0][6][0]
imdb_face_score2 = imdb[0][0][7][0]

wiki_photo_taken = wiki[0][0][1][0]
wiki_full_path = wiki[0][0][2][0]
wiki_gender = wiki[0][0][3][0]
wiki_face_score1 = wiki[0][0][6][0]
wiki_face_score2 = wiki[0][0][7][0]

# Generate lists with paths to images
imdb_path = []
wiki_path = []

for path in imdb_full_path:
    imdb_path.append('imdb_crop/' + path[0])
    
for path in wiki_full_path:
    wiki_path.append('wiki_crop/' + path[0])

# Generate lists for gender

imdb_genders = []
wiki_genders = []

for n in range(len(imdb_gender)):
    if imdb_gender[n] == 1:
        imdb_genders.append('male')
    else:
        imdb_genders.append('female')

for n in range(len(wiki_gender)):
    if wiki_gender[n] == 1:
        wiki_genders.append('male')
    else:
        wiki_genders.append('female')

imdb_dob = []
wiki_dob = []

for file in imdb_path:
    temp = file.split('_')[3]
    temp = temp.split('-')
    if len(temp[1]) == 1:
        temp[1] = '0' + temp[1]
    if len(temp[2]) == 1:
        temp[2] = '0' + temp[2]

    if temp[1] == '00':
        temp[1] = '01'
    if temp[2] == '00':
        temp[2] = '01'
    
    imdb_dob.append('-'.join(temp))

for file in wiki_path:
    wiki_dob.append(file.split('_')[2])

## Generate lists with ages

imdb_age = []
wiki_age = []

for i in range(len(imdb_dob)):
    try:
        d1 = date.datetime.strptime(imdb_dob[i][0:10], '%Y-%m-%d')
        d2 = date.datetime.strptime(str(imdb_photo_taken[i]), '%Y')
        rdelta = relativedelta(d2, d1)
        diff = rdelta.years
    except Exception as ex:
        #print(ex)
        diff = -1
    imdb_age.append(diff)
    
for i in range(len(wiki_dob)):
    try:
        d1 = date.datetime.strptime(wiki_dob[i][0:10], '%Y-%m-%d')
        d2 = date.datetime.strptime(str(wiki_photo_taken[i]), '%Y')
        rdelta = relativedelta(d2, d1)
        diff = rdelta.years
    except Exception as ex:
        #print(ex)
        diff = -1
    wiki_age.append(diff)

# Creating datasets with features and appropriate columns

final_imdb = np.vstack((imdb_age, imdb_genders, imdb_path, imdb_face_score1, imdb_face_score2)).T
final_wiki = np.vstack((wiki_age, wiki_genders, wiki_path, wiki_face_score1, wiki_face_score2)).T

final_imdb_df = pd.DataFrame(final_imdb)
final_wiki_df = pd.DataFrame(final_wiki)

columns = ['age', 'gender', 'path', 'face_score1', 'face_score2']

final_imdb_df.columns = columns
final_wiki_df.columns = columns

# Creating meta.csv with only age, gender and path

meta = pd.concat((final_imdb_df, final_wiki_df))

#remove pictures does not include face
meta = meta[meta['face_score1'] != '-inf']

#some pictures include more than one face, remove them
meta = meta[meta['face_score2'] == 'nan']

#drop columns
meta = meta.drop(['face_score1', 'face_score2'], axis=1)

meta = meta.sample(frac=1)

meta.to_csv('meta.csv', index=False)

"""# Pre-Processing"""

# Import Required Packages
import pandas as pd
import numpy as np
import numpy
import os
from sklearn.model_selection import train_test_split
import pylab as pl

# Import meta.csv

path = '/content/drive/My Drive/Master Data Science & Society/Thesis Data Science & Society/Data/meta.csv'
meta = pd.read_csv(path)

# Filtering dataset on invalid ages
meta = meta[meta['age'] >= 0]
meta = meta[meta['age'] <= 101]

# Save dataset without invalid ages in a new csv file
path_cor = '/content/drive/My Drive/Master Data Science & Society/Thesis Data Science & Society/Data/meta_cor.csv'
meta.to_csv(path_cor, index=False)

# Read corrected csv file
path_cor = '/content/drive/My Drive/Master Data Science & Society/Thesis Data Science & Society/Data/meta_cor.csv'
meta_cor = pd.read_csv(path_cor)

# Generate histogram with age distribution
histogram = meta_cor['age'].hist(bins=meta_cor['age'].nunique())
pl.title("Histogram of Age Distribution")
pl.xlabel("Age")
pl.ylabel("Number of individuals")

"""# Read Images & Ages from HDF5 file"""

# Enable to use file paths in drive
from google.colab import drive
drive.mount('/content/drive')

# Import Required Packages
import numpy
import h5py
from sklearn.model_selection import train_test_split

# Specify filename of dataset
fileName = '/content/drive/My Drive/Master Data Science & Society/Thesis Data Science & Society/Data/data_ssr-net_aa.hdf5'

# Read images and ages from the specific dataset

with h5py.File(fileName, "a") as out:
  images = numpy.asarray(out["X_train"])
  ages = numpy.asarray(out["Y_train"])

# Create histogram of age distribution of dataset (subsets)
from matplotlib import pyplot as plt 

b = []

for i in range(0,100):
  b.append(i)

plt.hist(ages, bins=b)
plt.title("Histogram of Age Distribution of Subset 1")
plt.xlabel("Age")
plt.ylabel("Number of individuals")
plt.grid()

# Create train and validation sets
X_train, X_test, y_train, y_test = train_test_split(images, ages, test_size=0.2, random_state=42)

"""# SSR-Net"""

# Import Required Packages

import logging
import sys
import numpy as np
from keras.models import Model
from keras.layers import Input, Activation, add, Dense, Flatten, Dropout, Multiply, Embedding, Lambda, Add, Concatenate, Activation
from keras.layers.convolutional import Conv2D, AveragePooling2D, MaxPooling2D
from keras.layers.normalization import BatchNormalization
from keras.regularizers import l2
from keras import backend as K
from keras.optimizers import SGD,Adam
from keras.utils import plot_model
from keras.engine.topology import Layer
from keras import activations, initializers, regularizers, constraints

sys.setrecursionlimit(2 ** 20)
np.random.seed(2 ** 10)

# Defining baseline model: RSS-Net

def model():
        logging.debug("Creating model...")


        inputs = Input(shape=(64,64,3))

##---------------------------------------------------------------------------------------------------------------------------------------------------##
##                                                                    SSR-Net                                                                        ##
##---------------------------------------------------------------------------------------------------------------------------------------------------##

  ## Stream 1 # channel size 32 ##
        
        # Stage 3
        x = Conv2D(32,(3,3))(inputs)
        x = BatchNormalization(axis=-1)(x)
        x = Activation('relu')(x)
        x_layer1 = AveragePooling2D(2,2)(x)

        # Stage 2
        x = Conv2D(32,(3,3))(x_layer1)
        x = BatchNormalization(axis=-1)(x)
        x = Activation('relu')(x)
        x_layer2 = AveragePooling2D(2,2)(x)

        # Stage 3
        x = Conv2D(32,(3,3))(x_layer2)
        x = BatchNormalization(axis=-1)(x)
        x = Activation('relu')(x)
        x_layer3 = AveragePooling2D(2,2)(x)
        # Block3_b
        x = Conv2D(32,(3,3))(x_layer3)
        x = BatchNormalization(axis=-1)(x)
        x = Activation('relu')(x)

#----------------------------------------------------------------------------------------------------------------------------------------------------
  ## Stream 2 # channel size 16 ##       
        
        # Block 1
        s = Conv2D(16,(3,3))(inputs)
        s = BatchNormalization(axis=-1)(s)
        s = Activation('tanh')(s)
        s_layer1 = MaxPooling2D(2,2)(s)

        # Block 2
        s = Conv2D(16,(3,3))(s_layer1)
        s = BatchNormalization(axis=-1)(s)
        s = Activation('tanh')(s)
        s_layer2 = MaxPooling2D(2,2)(s)

        # Block 3_a
        s = Conv2D(16,(3,3))(s_layer2)
        s = BatchNormalization(axis=-1)(s)
        s = Activation('tanh')(s)
        s_layer3 = MaxPooling2D(2,2)(s)
        # Block 3_b
        s = Conv2D(16,(3,3))(s_layer3)
        s = BatchNormalization(axis=-1)(s)
        s = Activation('tanh')(s)
        

##---------------------------------------------------------------------------------------------------------------------------------------------------##
##                                                                    Fusion Block & Regression                                                      ##                                                                        ##
##---------------------------------------------------------------------------------------------------------------------------------------------------##
## Fusion block & Regression # Stage 1 ##

        # Stream 2
        s_layer4 = Conv2D(10,(1,1),activation='relu')(s) #I
        s_layer4 = Flatten()(s_layer4)  
        s_layer4_mix = Dropout(0.2)(s_layer4) #II
        s_layer4_mix = Dense(units=3, activation="relu")(s_layer4_mix) #II
        
        # Stream 1
        x_layer4 = Conv2D(10,(1,1),activation='relu')(x) #I
        x_layer4 = Flatten()(x_layer4)
        x_layer4_mix = Dropout(0.2)(x_layer4) #II
        x_layer4_mix = Dense(units=3, activation="relu")(x_layer4_mix) #II

        # Delta Stage 1
        feat_a_s1_pre = Multiply()([s_layer4,x_layer4])
        delta_s1 = Dense(1,activation='tanh',name='delta_s1')(feat_a_s1_pre) #III
        
        # P stage 1
        feat_a_s1 = Multiply()([s_layer4_mix,x_layer4_mix])
        feat_a_s1 = Dense(2*3,activation='relu')(feat_a_s1)
        pred_a_s1 = Dense(units=3, activation="relu",name='pred_age_stage1')(feat_a_s1) #III
        #feat_local_s1 = Lambda(lambda x: x/10)(feat_a_s1)
        #feat_a_s1_local = Dropout(0.2)(pred_a_s1)
        
        # N stage 1
        local_s1 = Dense(units=3, activation='tanh', name='local_delta_stage1')(feat_a_s1) #III

#-----------------------------------------------------------------------------------------------------------------------------------------------------
## Fusion block & Regression # Stage 2 ##        
        
        # Stream 2
        s_layer2 = Conv2D(10,(1,1),activation='relu')(s_layer2) #I
        s_layer2 = MaxPooling2D(4,4)(s_layer2) #I
        s_layer2 = Flatten()(s_layer2)
        s_layer2_mix = Dropout(0.2)(s_layer2) #II
        s_layer2_mix = Dense(3,activation='relu')(s_layer2_mix) #II
        
        # Stream 1
        x_layer2 = Conv2D(10,(1,1),activation='relu')(x_layer2) #I
        x_layer2 = AveragePooling2D(4,4)(x_layer2) #I
        x_layer2 = Flatten()(x_layer2)
        x_layer2_mix = Dropout(0.2)(x_layer2) #II
        x_layer2_mix = Dense(3,activation='relu')(x_layer2_mix) #II

        # Delta stage 2
        feat_a_s2_pre = Multiply()([s_layer2,x_layer2])
        delta_s2 = Dense(1,activation='tanh',name='delta_s2')(feat_a_s2_pre) #III
        
        # P stage 2
        feat_a_s2 = Multiply()([s_layer2_mix,x_layer2_mix])
        feat_a_s2 = Dense(2*3,activation='relu')(feat_a_s2) #III
        pred_a_s2 = Dense(units=3, activation="relu",name='pred_age_stage2')(feat_a_s2) #III
        #feat_local_s2 = Lambda(lambda x: x/10)(feat_a_s2)
        #feat_a_s2_local = Dropout(0.2)(pred_a_s2)

        # N stage 2
        local_s2 = Dense(units=3, activation='tanh', name='local_delta_stage2')(feat_a_s2) #III

#-----------------------------------------------------------------------------------------------------------------------------------------------------
## Fusion block & Regression # Stage 3 ##
        
        # Stream 2
        s_layer1 = Conv2D(10,(1,1),activation='relu')(s_layer1) #I
        s_layer1 = MaxPooling2D(8,8)(s_layer1) #I
        s_layer1 = Flatten()(s_layer1)
        s_layer1_mix = Dropout(0.2)(s_layer1) #II
        s_layer1_mix = Dense(3,activation='relu')(s_layer1_mix) #II
        
        # Stream 1
        x_layer1 = Conv2D(10,(1,1),activation='relu')(x_layer1) #I
        x_layer1 = AveragePooling2D(8,8)(x_layer1) #I
        x_layer1 = Flatten()(x_layer1)
        x_layer1_mix = Dropout(0.2)(x_layer1) #II
        x_layer1_mix = Dense(3,activation='relu')(x_layer1_mix) #II

        # Delta stream 3
        feat_a_s3_pre = Multiply()([s_layer1,x_layer1]) 
        delta_s3 = Dense(1,activation='tanh',name='delta_s3')(feat_a_s3_pre) #III
        
        # P stream 3
        feat_a_s3 = Multiply()([s_layer1_mix,x_layer1_mix]) 
        feat_a_s3 = Dense(2*3,activation='relu')(feat_a_s3) #III
        pred_a_s3 = Dense(units=3, activation="relu",name='pred_age_stage3')(feat_a_s3) #III
        #feat_local_s3 = Lambda(lambda x: x/10)(feat_a_s3)
        #feat_a_s3_local = Dropout(0.2)(pred_a_s3)

        # N stream 3
        local_s3 = Dense(units=3, activation='tanh', name='local_delta_stage3')(feat_a_s3) #III

#----------------------------------------------------------------------------------------------------------------------------------------------------
  ## Age prediction ##
        
        # Function for age prediction
        def merge_age(x,s1,s2,s3,lambda_local,lambda_d):
            a = x[0][:,0]*0
            b = x[0][:,0]*0
            c = x[0][:,0]*0
            A = s1*s2*s3
            V = 101

            for i in range(0,s1):
                a = a+(i+lambda_local*x[6][:,i])*x[0][:,i]
            a = K.expand_dims(a,-1)
            a = a/(s1*(1+lambda_d*x[3]))

            for j in range(0,s2):
                b = b+(j+lambda_local*x[7][:,j])*x[1][:,j]
            b = K.expand_dims(b,-1)
            b = b/(s1*(1+lambda_d*x[3]))/(s2*(1+lambda_d*x[4]))

            for k in range(0,s3):
                c = c+(k+lambda_local*x[8][:,k])*x[2][:,k]
            c = K.expand_dims(c,-1)
            c = c/(s1*(1+lambda_d*x[3]))/(s2*(1+lambda_d*x[4]))/(s3*(1+lambda_d*x[5]))


            age = (a+b+c)*V
            return age

#----------------------------------------------------------------------------------------------------------------------------------------------------      
## Output layer which calls age prediction function # combining all stages ##

        pred_a = Lambda(merge_age,arguments={'s1':3,'s2':3,'s3':3,'lambda_local':1,'lambda_d':1},output_shape=(1,),name='pred_a')([pred_a_s1,pred_a_s2,pred_a_s3,delta_s1,delta_s2,delta_s3, local_s1, local_s2, local_s3])

#----------------------------------------------------------------------------------------------------------------------------------------------------
## Model creation ##

        model = Model(inputs=inputs, outputs=pred_a)

        return model

age_model = model()

# Load trained weights
#path_weights_ssr = "/content/drive/My Drive/Master Data Science & Society/Thesis Data Science & Society/Code/Weights_SSR-Net_30_10.h5"
#age_model.load_weights(path_weights_ssr)

#Freeze weights in age_model
for layer in age_model.layers[:]:
  layer.trainable = False

#age_model.summary()

# Compile model
optMethod = Adam()
age_model.compile(optimizer=optMethod, loss=["mae"], metrics={'pred_a':'mae'})

# Evaluete model for training score
age_model.evaluate(X_train, y_train, batch_size=128, verbose=1)

# Evaluate model for validation score
age_model.evaluate(X_test, y_test, batch_size=128, verbose=1)

"""Train SSR-Net weights"""

optMethod = Adam()

age_model.compile(optimizer=optMethod, loss=["mae"], metrics={'pred_a':'mae'})

import matplotlib.pyplot as plt

history = age_model.fit(X_train, y_train, epochs=90, batch_size=128, validation_data=(X_test, y_test) )

# serialize weights to HDF5
age_model.save_weights("/content/drive/My Drive/Master Data Science & Society/Thesis Data Science & Society/Code/Weights_SSRN_90_50.h5")

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

age_model.evaluate(X_train, y_train, batch_size=128, verbose=1)

age_model.evaluate(X_test, y_test, batch_size=128, verbose=1)

"""# Attention Augmented Convolutional Networks"""

# Import Required Packages

from keras.layers import Layer, Conv2D, concatenate
from keras import initializers
from keras import backend as K
import tensorflow as tf

# Function for creating a Conv2D layer

def _conv_layer(filters, kernel_size, strides=(1, 1), padding='same', name=None):
    return Conv2D(filters, kernel_size, strides=strides, padding=padding, use_bias=True, kernel_initializer='he_normal', name=name)

# Function for normalization of depth_vars

def _normalize_depth_vars(depth_k, depth_v, filters):
    if type(depth_k) == float:
        depth_k = int(filters * depth_k)
    else:
        depth_k = int(depth_k)

    if type(depth_v) == float:
        depth_v = int(filters * depth_v)
    else:
        depth_v = int(depth_v)

    return depth_k, depth_v

"""## **Attention Augmentation Layer**"""

class AttentionAugmentation2D(Layer):

#-----------------------------------------------------------------------------------------------------------------------------------------------------

  def __init__(self, depth_k, depth_v, num_heads, relative=True, **kwargs):
        
        # Reference to the parent class: AttentionAugmentation2D
        super(AttentionAugmentation2D, self).__init__(**kwargs)

        #----------------------------------------------------------------------------------------
        # Raises ValueError if depth_v or depth_k is not divisible by num_heads
        if depth_k % num_heads != 0:
            raise ValueError('`depth_k` (%d) is not divisible by `num_heads` (%d)' % (
                depth_k, num_heads))

        if depth_v % num_heads != 0:
            raise ValueError('`depth_v` (%d) is not divisible by `num_heads` (%d)' % (
                depth_v, num_heads))

        if depth_k // num_heads < 1.:
            raise ValueError('depth_k / num_heads cannot be less than 1 ! '
                             'Given depth_k = %d, num_heads = %d' % (
                             depth_k, num_heads))

        if depth_v // num_heads < 1.:
            raise ValueError('depth_v / num_heads cannot be less than 1 ! '
                             'Given depth_v = %d, num_heads = %d' % (
                                 depth_v, num_heads))
            
        #----------------------------------------------------------------------------------------
        # Get parameters from function call; store in 'self' (?)
        self.depth_k = depth_k        # Number of filters for k; computes the number of filters for 'v'
        self.depth_v = depth_v        # Number of filters for v; computes the number of filters for 'k'
        self.num_heads = num_heads    # Number of attention heads (see errors section above)
        self.relative = relative      # Whether to use relative encodings

        # Set axis depending on image data format -> format = 'channels_last'
        self.axis = 1 if K.image_data_format() == 'channels_first' else -1

#-----------------------------------------------------------------------------------------------------------------------------------------------------
  ## Get information from input shape  # Dependend on image data format ##

  def build(self, input_shape):
        
        # Get input_shape from function call; store in 'self' (?)
        self._shape = input_shape

        # Normalize the format of depth_v and depth_k
        self.depth_k, self.depth_v = _normalize_depth_vars(self.depth_k, self.depth_v, input_shape)

        # Set sequence of input_shape parameters
        if self.axis == 1:
            _, channels, height, width = input_shape
        else:
            _, height, width, channels = input_shape

        #----------------------------------------------------------------------------------------
        # Refers to 'self.relative=True'
        if self.relative:
            dk_per_head = self.depth_k // self.num_heads

            if dk_per_head == 0:
                print('dk per head', dk_per_head)

            self.key_relative_w = self.add_weight('key_rel_w',
                                                  shape=[2 * width - 1, dk_per_head],
                                                  initializer=initializers.RandomNormal(
                                                      stddev=dk_per_head ** -0.5))

            self.key_relative_h = self.add_weight('key_rel_h',
                                                  shape=[2 * height - 1, dk_per_head],
                                                  initializer=initializers.RandomNormal(
                                                      stddev=dk_per_head ** -0.5))
        # Refers to 'self.relative=False'
        else:
            self.key_relative_w = None
            self.key_relative_h = None

#-----------------------------------------------------------------------------------------------------------------------------------------------------
  ## Create frame for output tensor # Dependend on computations in previous section ##

  def call(self, inputs, **kwargs):
        
        # Only executed if data format is 'channels first'
        if self.axis == 1:
            # If channels first, force it to be channels last for these ops
            inputs = K.permute_dimensions(inputs, [0, 2, 3, 1])

        #------------------------------------------------------------------------------------------------------------
        # Split the input tensor into sub tensors
        q, k, v = tf.split(inputs, [self.depth_k, self.depth_k, self.depth_v], axis=-1)

        # 'split_heads_2d' function is created later
        q = self.split_heads_2d(q)      # refers to query
        k = self.split_heads_2d(k)      # refers to key
        v = self.split_heads_2d(v)      # refers to value

        #------------------------------------------------------------------------------------------------------------
        # Scale query
        depth_k_heads = self.depth_k / self.num_heads
        q *= (depth_k_heads ** -0.5)

        #------------------------------------------------------------------------------------------------------------
        # Get shape of query|key and value tensor & Reshape

        # [Batch, num_heads, height * width, depth_k or depth_v] if axis == -1
        qk_shape = [self._batch, self.num_heads, self._height * self._width, self.depth_k // self.num_heads]
        v_shape = [self._batch, self.num_heads, self._height * self._width, self.depth_v // self.num_heads]
        
        # Reshape
        flat_q = K.reshape(q, K.stack(qk_shape))
        flat_k = K.reshape(k, K.stack(qk_shape))
        flat_v = K.reshape(v, K.stack(v_shape))

        #------------------------------------------------------------------------------------------------------------
        # Create attention output frame

        # [Batch, num_heads, HW, HW]
        logits = tf.matmul(flat_q, flat_k, transpose_b=True)

        # Apply relative encodings ('relative_logits' function created later)
        if self.relative:
            h_rel_logits, w_rel_logits = self.relative_logits(q)
            logits += h_rel_logits
            logits += w_rel_logits

        weights = K.softmax(logits, axis=-1)
        attn_out = tf.matmul(weights, flat_v)

        attn_out_shape = [self._batch, self.num_heads, self._height, self._width, self.depth_v // self.num_heads]
        attn_out_shape = K.stack(attn_out_shape)
        attn_out = K.reshape(attn_out, attn_out_shape)
        attn_out = self.combine_heads_2d(attn_out)
        # [batch, height, width, depth_v]

        #------------------------------------------------------------------------------------------------------------
        # Only executed if data format is 'channels first'
        if self.axis == 1:
            # return to [batch, depth_v, height, width] for channels first
            attn_out = K.permute_dimensions(attn_out, [0, 3, 1, 2])

        #------------------------------------------------------------------------------------------------------------
        return attn_out

#-----------------------------------------------------------------------------------------------------------------------------------------------------
  ## Compute output shape of attention tensor ##

  def compute_output_shape(self, input_shape):
        output_shape = list(input_shape)
        output_shape[self.axis] = self.depth_v
        return tuple(output_shape)

#-----------------------------------------------------------------------------------------------------------------------------------------------------
  ## 'split_heads_2d' function used previously to split q, k, v sub tensors ##

  def split_heads_2d(self, ip):
        
        # Get input shape provided in function call (ip)
        tensor_shape = K.shape(ip)

        # Batch, height, width, channels for axis = -1
        tensor_shape = [tensor_shape[i] for i in range(len(self._shape))]

        batch = tensor_shape[0]         # batch size of input
        height = tensor_shape[1]        # height of input
        width = tensor_shape[2]         # width of input
        channels = tensor_shape[3]      # number of channels

        # Save the spatial tensor dimensions
        self._batch = batch
        self._height = height
        self._width = width

        ret_shape = K.stack([batch, height, width,  self.num_heads, channels // self.num_heads])
        split = K.reshape(ip, ret_shape)
        transpose_axes = (0, 3, 1, 2, 4)
        split = K.permute_dimensions(split, transpose_axes)

        return split

#-----------------------------------------------------------------------------------------------------------------------------------------------------
  ## 'relative_logits' function used previously

  def relative_logits(self, q):
        # Get shape of query tensor
        shape = K.shape(q)

        # [batch, num_heads, H, W, depth_v]
        shape = [shape[i] for i in range(5)]

        height = shape[2]       # height of query tensor
        width = shape[3]        # width of query tensor

        # Relative logits for width (function in next section used)
        rel_logits_w = self.relative_logits_1d(q, self.key_relative_w, height, width, transpose_mask=[0, 1, 2, 4, 3, 5])
        
        # Relative logits for heigth (function in next section used)
        rel_logits_h = self.relative_logits_1d(
            K.permute_dimensions(q, [0, 1, 3, 2, 4]),
            self.key_relative_h, width, height,
            transpose_mask=[0, 1, 4, 2, 5, 3])

        return rel_logits_h, rel_logits_w

#-----------------------------------------------------------------------------------------------------------------------------------------------------
  ## 'relative_logits_1d' function # Used to compute relative logits of width and height in previous section ##

  def relative_logits_1d(self, q, rel_k, H, W, transpose_mask):
        rel_logits = tf.einsum('bhxyd,md->bhxym', q, rel_k)
        rel_logits = K.reshape(rel_logits, [-1, self.num_heads * H, W, 2 * W - 1])
        rel_logits = self.rel_to_abs(rel_logits)
        rel_logits = K.reshape(rel_logits, [-1, self.num_heads, H, W, W])
        rel_logits = K.expand_dims(rel_logits, axis=3)
        rel_logits = K.tile(rel_logits, [1, 1, 1, H, 1, 1])
        rel_logits = K.permute_dimensions(rel_logits, transpose_mask)
        rel_logits = K.reshape(rel_logits, [-1, self.num_heads, H * W, H * W])
        return rel_logits

#-----------------------------------------------------------------------------------------------------------------------------------------------------
  ## 'rel_to_abs' function # Used in 'relative_logits_1d' ##

  def rel_to_abs(self, x):
        shape = K.shape(x)
        shape = [shape[i] for i in range(3)]
        B, Nh, L, = shape
        col_pad = K.zeros(K.stack([B, Nh, L, 1]))
        x = K.concatenate([x, col_pad], axis=3)
        flat_x = K.reshape(x, [B, Nh, L * 2 * L])
        flat_pad = K.zeros(K.stack([B, Nh, L - 1]))
        flat_x_padded = K.concatenate([flat_x, flat_pad], axis=2)
        final_x = K.reshape(flat_x_padded, [B, Nh, L + 1, 2 * L - 1])
        final_x = final_x[:, :, :L, L - 1:]
        return final_x

#-----------------------------------------------------------------------------------------------------------------------------------------------------
  ## Combine splitted heads again ##

  def combine_heads_2d(self, inputs):
        # [batch, num_heads, height, width, depth_v // num_heads]
        transposed = K.permute_dimensions(inputs, [0, 2, 3, 1, 4])
        # [batch, height, width, num_heads, depth_v // num_heads]
        shape = K.shape(transposed)
        shape = [shape[i] for i in range(5)]

        a, b = shape[-2:]
        ret_shape = K.stack(shape[:-2] + [a * b])
        # [batch, height, width, depth_v]
        return K.reshape(transposed, ret_shape)

#-----------------------------------------------------------------------------------------------------------------------------------------------------
  ## Get configuration of parameters # Reference to the class ###

  def get_config(self):
        config = {
            'depth_k': self.depth_k,
            'depth_v': self.depth_v,
            'num_heads': self.num_heads,
            'relative': self.relative,
        }
        base_config = super(AttentionAugmentation2D, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
#-----------------------------------------------------------------------------------------------------------------------------------------------------

"""# Augmented convolution block"""

## Function to build augmented convolution block ##

def augmented_conv2d(ip, filters, kernel_size=(3, 3), strides=(1, 1),
                     depth_k=0.2, depth_v=0.2, num_heads=8, relative_encodings=True):
  #------------------------------------------------------------------------------------------------------------------
    # Set channel axis depending on data format
    
    # input_shape = K.int_shape(ip)
    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1
  
  #------------------------------------------------------------------------------------------------------------------
    # Normalize the format of depth_v and depth_k
    depth_k, depth_v = _normalize_depth_vars(depth_k, depth_v, filters)

  #------------------------------------------------------------------------------------------------------------------
    # Convolution part before augmented attention layer
    conv_out = _conv_layer(filters - depth_v, kernel_size, strides, name='conv_out')(ip)

  #------------------------------------------------------------------------------------------------------------------
    # Augmented Attention Block

    # Input previous convolution layer 
    qkv_conv = _conv_layer(2 * depth_k + depth_v, (1, 1), strides, name='qkv_conv')(ip)   # filters, kernel_size, strides, name

    # Attention augmentation block -> build with 'AttentionAugmentation2D' class
    attn_out = AttentionAugmentation2D(depth_k, depth_v, num_heads, relative_encodings)(qkv_conv)
    
    # Output attn_out layer as convolutional kernel
    attn_out = _conv_layer(depth_v, kernel_size=(1, 1), name='attn_out')(attn_out)

    # Concatenate convolution blocks with Augmented attention
    output = concatenate([conv_out, attn_out], axis=channel_axis)
    
    #------------------------------------------------------------------------------------------------------------------
    
    return output

"""# Augmented Attention SSR-Net"""

def AA_SSR_Net(filters=40, kernel_size=(3, 3), strides=(1, 1), depth_k=0.2, depth_v=0.2, num_heads=8, relative_encodings=True):

  inputs = Input(shape=(64,64,3))

##---------------------------------------------------------------------------------------------------------------------------------------------------##
##                                                                    SSR-Net                                                                        ##
##---------------------------------------------------------------------------------------------------------------------------------------------------##

  ## Stream 1 # with AA in stage 1 ##

  # Stage 3
  x = Conv2D(32,(3,3))(inputs)
  x = BatchNormalization(axis=-1)(x)
  x = Activation('relu')(x)
  x_layer1 = AveragePooling2D(2,2)(x)

  # Stage 2
  x = Conv2D(32,(3,3))(x_layer1)
  x = BatchNormalization(axis=-1)(x)
  x = Activation('relu')(x)
  x_layer2 = AveragePooling2D(2,2)(x)

  #-----------------------------------------------------------------------------------------------------------

  # Stage 1_a
  x = Conv2D(24,(1,1))(x_layer2)
  x = BatchNormalization(axis=-1)(x)
  x_aa_conv = Activation('relu')(x)
  #x_aa_conv = AveragePooling2D(2,2)(x)

  # Prepare input for attention
  channel_axis = 1 if K.image_data_format() == 'channels_first' else -1
  depth_k,  depth_v = _normalize_depth_vars(depth_k, depth_v, filters)

  # Stage 1: Augmented attention
  x_qkv = _conv_layer(2 * depth_k + depth_v, (1, 1), strides)(x_layer2)
  x_attn_out = AttentionAugmentation2D(depth_k, depth_v,num_heads, relative_encodings)(x_qkv)
  x_attn_out = _conv_layer(depth_v, kernel_size=(3, 3))(x_attn_out)

  # Concatenate Stage 3
  x_layer3 = concatenate([x_aa_conv, x_attn_out], axis=channel_axis)
  x_layer3= BatchNormalization(axis=-1)(x_layer3)
  x_layer3 = AveragePooling2D(2,2)(x_layer3)

  #-----------------------------------------------------------------------------------------------------------
  # Stage 1_b
  x = Conv2D(32,(3,3))(x_layer3)
  x = BatchNormalization(axis=-1)(x)
  x = Activation('relu')(x)

#----------------------------------------------------------------------------------------------------------------------------------------------------
  ## Stream 2 # with AA in stage3 ##

  # Stage 3: SSR-Net
  s = Conv2D(16,(3,3))(inputs)
  s = BatchNormalization(axis=-1)(s)
  s = Activation('tanh')(s)
  s_layer1 = MaxPooling2D(2,2)(s)

  # Block 2
  s = Conv2D(16,(3,3))(s_layer1)
  s = BatchNormalization(axis=-1)(s)
  s = Activation('tanh')(s)
  s_layer2 = MaxPooling2D(2,2)(s)

  #-----------------------------------------------------------------------------------------------------------

  # Prepare input for attention
  channel_axis = 1 if K.image_data_format() == 'channels_first' else -1
  depth_k,  depth_v = _normalize_depth_vars(depth_k, depth_v, filters)

  # Block 3_a
  s = Conv2D(8,(1,1))(s_layer2)
  s = BatchNormalization(axis=-1)(s)
  s_conv = Activation('tanh')(s)
  #s_layer3 = MaxPooling2D(2,2,name='input')(s)

  # Stage 1: Augmented attention
  s_qkv = _conv_layer(2 * depth_k + depth_v, (1, 1), strides)(s_layer2)
  s_attn_out = AttentionAugmentation2D(depth_k, depth_v,num_heads, relative_encodings)(s_qkv)
  s_attn_out = _conv_layer(depth_v, kernel_size=(1, 1))(s_attn_out)

  # Concatenate Stage 3
  s_layer3 = concatenate([s_conv, s_attn_out], axis=channel_axis)
  s_layer3 = BatchNormalization(axis=-1)(s_layer3)
  s_layer3 = MaxPooling2D(2,2)(s_layer3)

  #-----------------------------------------------------------------------------------------------------------
  
  # Block 3_b
  s = Conv2D(16,(3,3))(s_layer3)
  s = BatchNormalization(axis=-1)(s)
  s = Activation('tanh')(s)

##---------------------------------------------------------------------------------------------------------------------------------------------------##
##                                                                    Fusion Block & Regression                                                      ##                                                                        ##
##---------------------------------------------------------------------------------------------------------------------------------------------------##
## Fusion block & Regression # Stage 1 ##

  # Stream 2
  s_layer4 = Conv2D(10,(1,1),activation='relu')(s) #I
  s_layer4 = Flatten()(s_layer4)  
  s_layer4_mix = Dropout(0.2)(s_layer4) #II
  s_layer4_mix = Dense(units=3, activation="relu")(s_layer4_mix) #II
        
  # Stream 1
  x_layer4 = Conv2D(10,(1,1),activation='relu')(x) #I
  x_layer4 = Flatten()(x_layer4)
  x_layer4_mix = Dropout(0.2)(x_layer4) #II
  x_layer4_mix = Dense(units=3, activation="relu")(x_layer4_mix) #II

  # Delta Stage 1
  feat_a_s1_pre = Multiply()([s_layer4,x_layer4])
  delta_s1 = Dense(1,activation='tanh',name='delta_s1')(feat_a_s1_pre) #III
        
  # P stage 1
  feat_a_s1 = Multiply()([s_layer4_mix,x_layer4_mix])
  feat_a_s1 = Dense(2*3,activation='relu')(feat_a_s1)
  pred_a_s1 = Dense(units=3, activation="relu",name='pred_age_stage1')(feat_a_s1) #III
  #feat_local_s1 = Lambda(lambda x: x/10)(feat_a_s1)
  #feat_a_s1_local = Dropout(0.2)(pred_a_s1)
        
  # N stage 1
  local_s1 = Dense(units=3, activation='tanh', name='local_delta_stage1')(feat_a_s1) #III

#-----------------------------------------------------------------------------------------------------------------------------------------------------
## Fusion block & Regression # Stage 2 ##        
        
  # Stream 2
  s_layer2 = Conv2D(10,(1,1),activation='relu')(s_layer2) #I
  s_layer2 = MaxPooling2D(4,4)(s_layer2) #I
  s_layer2 = Flatten()(s_layer2)
  s_layer2_mix = Dropout(0.2)(s_layer2) #II
  s_layer2_mix = Dense(3,activation='relu')(s_layer2_mix) #II
        
  # Stream 1
  x_layer2 = Conv2D(10,(1,1),activation='relu')(x_layer2) #I
  x_layer2 = AveragePooling2D(4,4)(x_layer2) #I
  x_layer2 = Flatten()(x_layer2)
  x_layer2_mix = Dropout(0.2)(x_layer2) #II
  x_layer2_mix = Dense(3,activation='relu')(x_layer2_mix) #II

  # Delta stage 2
  feat_a_s2_pre = Multiply()([s_layer2,x_layer2])
  delta_s2 = Dense(1,activation='tanh',name='delta_s2')(feat_a_s2_pre) #III
        
  # P stage 2
  feat_a_s2 = Multiply()([s_layer2_mix,x_layer2_mix])
  feat_a_s2 = Dense(2*3,activation='relu')(feat_a_s2) #III
  pred_a_s2 = Dense(units=3, activation="relu",name='pred_age_stage2')(feat_a_s2) #III
  #feat_local_s2 = Lambda(lambda x: x/10)(feat_a_s2)
  #feat_a_s2_local = Dropout(0.2)(pred_a_s2)

  # N stage 2
  local_s2 = Dense(units=3, activation='tanh', name='local_delta_stage2')(feat_a_s2) #III

#-----------------------------------------------------------------------------------------------------------------------------------------------------
## Fusion block & Regression # Stage 3 ##
        
  # Stream 2
  s_layer1 = Conv2D(10,(1,1),activation='relu')(s_layer1) #I
  s_layer1 = MaxPooling2D(8,8)(s_layer1) #I
  s_layer1 = Flatten()(s_layer1)
  s_layer1_mix = Dropout(0.2)(s_layer1) #II
  s_layer1_mix = Dense(3,activation='relu')(s_layer1_mix) #II
        
  # Stream 1
  x_layer1 = Conv2D(10,(1,1),activation='relu')(x_layer1) #I
  x_layer1 = AveragePooling2D(8,8)(x_layer1) #I
  x_layer1 = Flatten()(x_layer1)
  x_layer1_mix = Dropout(0.2)(x_layer1) #II
  x_layer1_mix = Dense(3,activation='relu')(x_layer1_mix) #II

  # Delta stream 3
  feat_a_s3_pre = Multiply()([s_layer1,x_layer1]) 
  delta_s3 = Dense(1,activation='tanh',name='delta_s3')(feat_a_s3_pre) #III
        
  # P stream 3
  feat_a_s3 = Multiply()([s_layer1_mix,x_layer1_mix]) 
  feat_a_s3 = Dense(2*3,activation='relu')(feat_a_s3) #III
  pred_a_s3 = Dense(units=3, activation="relu",name='pred_age_stage3')(feat_a_s3) #III
  #feat_local_s3 = Lambda(lambda x: x/10)(feat_a_s3)
  #feat_a_s3_local = Dropout(0.2)(pred_a_s3)

  # N stream 3
  local_s3 = Dense(units=3, activation='tanh', name='local_delta_stage3')(feat_a_s3) #III

#----------------------------------------------------------------------------------------------------------------------------------------------------
  ## Age prediction ##
        
  # Function for age prediction
  def merge_age(x,s1,s2,s3,lambda_local,lambda_d):
      a = x[0][:,0]*0
      b = x[0][:,0]*0
      c = x[0][:,0]*0
      A = s1*s2*s3
      V = 101

      for i in range(0,s1):
          a = a+(i+lambda_local*x[6][:,i])*x[0][:,i]
      a = K.expand_dims(a,-1)
      a = a/(s1*(1+lambda_d*x[3]))

      for j in range(0,s2):
          b = b+(j+lambda_local*x[7][:,j])*x[1][:,j]
      b = K.expand_dims(b,-1)
      b = b/(s1*(1+lambda_d*x[3]))/(s2*(1+lambda_d*x[4]))

      for k in range(0,s3):
          c = c+(k+lambda_local*x[8][:,k])*x[2][:,k]
      c = K.expand_dims(c,-1)
      c = c/(s1*(1+lambda_d*x[3]))/(s2*(1+lambda_d*x[4]))/(s3*(1+lambda_d*x[5]))


      age = (a+b+c)*V
      return age

#----------------------------------------------------------------------------------------------------------------------------------------------------      
## Output layer which calls age prediction function # combining all stages ##

  pred_a = Lambda(merge_age,arguments={'s1':3,'s2':3,'s3':3,'lambda_local':1,'lambda_d':1},output_shape=(1,),name='pred_a')([pred_a_s1,pred_a_s2,pred_a_s3,delta_s1,delta_s2,delta_s3, local_s1, local_s2, local_s3])

#----------------------------------------------------------------------------------------------------------------------------------------------------
## Model creation ##

  model = Model(inputs=inputs, outputs=pred_a)

  return model

model = AA_SSR_Net()

# Load trained weights
#path_weights_ssr = "/content/drive/My Drive/Master Data Science & Society/Thesis Data Science & Society/Code/Weights_SSR_AA_30_10.h5"
#model.load_weights(path_weights_ssr)

#Freeze weights in age_model
for layer in model.layers[:]:
  layer.trainable = False

#model.summary()

# Compile model
optMethod = Adam()
model.compile(optimizer=optMethod, loss=["mae"], metrics={'pred_a':'mae'})

# Evaluate for training score
model.evaluate(X_train, y_train, batch_size=128, verbose=1)

# Evaluate for validation score
model.evaluate(X_test, y_test, batch_size=128, verbose=1)

"""Train Attention Augmented Convoluton SSR-Net"""

# Fitting model
history = model.fit(X_train, y_train, epochs=30, batch_size=128, validation_data=(X_test, y_test) )

# serialize weights to HDF5
model.save_weights("/content/drive/My Drive/Master Data Science & Society/Thesis Data Science & Society/Code/Weights_SSR_AA_30_10.h5")

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Mean Absolute Error')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()